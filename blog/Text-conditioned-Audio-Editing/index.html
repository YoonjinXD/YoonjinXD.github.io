<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Text-conditioned Audio Editing" /><meta property="og:locale" content="en" /><meta name="description" content="2022 Fall AI605 Final Project, Yoonjin Chung, Junwon Lee" /><meta property="og:description" content="2022 Fall AI605 Final Project, Yoonjin Chung, Junwon Lee" /><link rel="canonical" href="https://yoonjinxd.github.io/blog/Text-conditioned-Audio-Editing/" /><meta property="og:url" content="https://yoonjinxd.github.io/blog/Text-conditioned-Audio-Editing/" /><meta property="og:site_name" content="Yoonjin" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-02T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Text-conditioned Audio Editing" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-04-13T15:12:17+00:00","datePublished":"2023-03-02T00:00:00+00:00","description":"2022 Fall AI605 Final Project, Yoonjin Chung, Junwon Lee","headline":"Text-conditioned Audio Editing","mainEntityOfPage":{"@type":"WebPage","@id":"https://yoonjinxd.github.io/blog/Text-conditioned-Audio-Editing/"},"url":"https://yoonjinxd.github.io/blog/Text-conditioned-Audio-Editing/"}</script><title>Text-conditioned Audio Editing | Yoonjin</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Yoonjin"><meta name="application-name" content="Yoonjin"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.20.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-topbar-visible="true"><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper" style="text-align: center;"><h1 class="site-title"> <a href="/">Yoonjin</a></h1><p class="site-subtitle fst-italic mb-0">Audio AI</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-pen"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag"></i> <span>TAGS</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/YoonjinXD" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/yoon-jin-chung-03783016a/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="https://scholar.google.com/citations?user=UXaiGH4AAAAJ&hl=en" aria-label="google-scholar" target="_blank" rel="noopener noreferrer" > <i class="fa fa-graduation-cap"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fab fa-twitter"></i> </a></div></aside><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Text-conditioned Audio Editing</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Text-conditioned Audio Editing</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1677715200" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Mar 2, 2023 </em> </span> <span> Updated <em class="" data-ts="1713021137" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Apr 14, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/yoonjinxd">Yoonjin Chung</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="999 words"> <em>5 min</em> read</span></div></div></div><div class="post-content"><p><em>2022 Fall AI605 Final Project</em>, Yoonjin Chung, Junwon Lee</p><p>github link: <a href="https://github.com/YoonjinXD/Text-conditioned-Audio-Editing">https://github.com/YoonjinXD/Text-conditioned-Audio-Editing</a></p><p>report: <a href="https://github.com/YoonjinXD/Text-conditioned-Audio-Editing/blob/master/AI605_Capstone_Report.pdf">https://github.com/YoonjinXD/Text-conditioned-Audio-Editing/blob/master/AI605_Capstone_Report.pdf</a></p><hr /><h1 id="introduction">Introduction</h1><p>Since finding specific data that satisfy various conditions in the real world, generating new data by editing samples has been a goal to achieve and a challenging task as well. In this study, we address the text-conditioned audio editing problem for the first time, to the best of our knowledge, by fine-tuning and interpolating text embeddings of audio captions. Our task aims to generate target sounds that are slightly modified from existing audio referring to given caption conditions.</p><p><a href="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/figure_1.png" class="popup img-link "><img data-src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/figure_1.png" class="lazyload" data-proofer-ignore></a></p><ul><li>We address the text-conditioned audio editing task, which has rarely been studied.<li>We optimize the given text embedding and fine-tune the discrete diffusion model to generate semantically edited sound.<li>We provide the qualitative analysis on caption prompts and interpolation intensity by comparing our results from various types of prompts.</ul><h1 id="related-works">Related Works</h1><p><strong>Diffsound(Yang et al., 2022)</strong> is the first model that solves the text-to-audio generation task directly. The proposed a discrete-diffusion-based audio generative model conditioned on a text prompt. The model consists of a pretrained CLIP text encoder, a discrete diffusion model, a Vector Quantized Variational Autoencoder(VQ-VAE) and a vocoder. After the text prompt is encoded as a feature, the diffusion decoder transfers the text feature to a sequence of quantized tokens which is passed to the pretrained VQ-VAE decoder to reconstruct a mel-spectrogram. Finally, the vocoder generates a waveform from the transferred mel-spectrogram.</p><p><strong>Imagic(Kawar et al., 2022)</strong> is the only model, that tackles the image editing task conditioned on a text prompt. Given a single target text prompt and image pair, the model is fine-tuned in several steps. As the given prompt does not describe the original image but the change to be applied, only fine-tune the text encoder first on text-image pair while freezing others, to get optimized embedding which is distinct from the target embedding acquired from original text encoder in Imagen.To get the edited image, they finally input the embedding which is a linear interpolation between the target and optimized embeddings. The authors showed the interpolation is semantically valid in a qualitative manner by providing examples corresponding to several editing categories.</p><h1 id="proposed-method">Proposed Method</h1><h2 id="architecture"><span class="mr-2">Architecture</span><a href="#architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We exploit pretrained Diffsound model as our base framework as it is the only text-to-audio model which is open-source. As we finetune only some part of Diffsound initializing it with provided pretrained weights, the overall architecture is identical. See Fig 1 to see a simply summarized diagram. Among the components, we do not fintune VQ-VAE, the decoder part that generates mel spectrogram from a sequence of quantized mel spectrogram tokens, and MelGAN, the vocoder, which is both trained on Audioset respectively. The two are not involved in processing text caption but in generating the audio sound. In this paper, we focus on CLIP which encodes the text prompt, and discrete diffusion model which generates a quantized token sequence conditioned on the text embedding.</p><p><a href="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/figure_2.png" class="popup img-link "><img data-src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/figure_2.png" class="lazyload" data-proofer-ignore></a></p><h2 id="methods"><span class="mr-2">Methods</span><a href="#methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We follow the proposed method suggested in Imagic (Kawar et al., 2022) to achieve our goal. Given the original input audio, we aim to output the edited audio through the semantics included in the text prompt. Note that the prompt is not a caption that describes the original audio but the desirable modified version of it. Let the text representation which encodes this prompt be a target embedding. As there is no way for the text-to-audio model to generate audio similar to the original one, the key idea is to find an optimized embedding that enables it when passed through the generative layers.</p><p><strong>The brief procedure is the following:</strong></p><ol><li>We finetune CLIP text embedding layer to acquire an optimized embedding vector near the target text embedding. The ideal optimized text embedding, as an input of the audio generative model, may nearly reconstruct the original audio.<li>We finetune the discrete diffusion model, with optimized embedding representation as input, to generate the original audio better.<li>Compute the linear interpolation of the target embedding and the optimized embedding and input to the generation part. Proper interpolation enables to find a representation that maintains balance between audio fidelity and text prompt alignment. which is described visually in Fig 2.</ol><h1 id="result">Result</h1><p>We have verified that substitution in sound source (i.e. who/what is making sound, which kind of sound has occurred) is successful while preserving the structural feature of the audio.</p><p><a href="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/result.png" class="popup img-link "><img data-src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/result.png" class="lazyload" data-proofer-ignore></a></p><h3 id="sample-1"><span class="mr-2">Sample 1</span><a href="#sample-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>(Original)A car horn sounds loudly and then fades away → (Edited)A bell sounds loudly and then fades away</p><div class="table-wrapper"><table><tr><th>Original<th>Editied<tr><td><audio src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/Y2KCoO8C8R8.wav" controls=""></audio><td><audio src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/Y2KCoO8C8R8_G_0.6.wav" controls=""></audio></table></div><h3 id="sample-2"><span class="mr-2">Sample 2</span><a href="#sample-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>(Original)A woman gives a speech → (Edited)A man gives a speech</p><div class="table-wrapper"><table><tr><th>Original<th>Editied<tr><td><audio src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/1wOcbw5Rg84.wav" controls=""></audio><td><audio src="https://yoonjinxd.github.io/images/2023-03-02-Text-conditioned-Audio-Editing/1wOcbw5Rg84_G_0.5.wav" controls=""></audio></table></div><p><br /></p><p>As you can see from the given samples, it is able to edit the horn to the bell sound while maintaining a structure in which the bell rings once loudly and then fades away as same as the original audio.</p><h1 id="limitations">Limitations</h1><p>Some edits are applied very subtly, so that they do not align well with the target text. Since the audio generative diffusion model that we used is only trained with ontological tags from Audioset, it seems that cannot capture complex semantics such as temporal information(e.g. after, then) or spatial information(e.g. foreground, background) in the audio domain.</p><h1 id="conclusion--future-work">Conclusion &amp; Future Work</h1><p>In this study, we propose the first text-conditioned audio editing method based on the pre-trained generative diffusion model. With our method, an existing audio and caption pair describing the desired editing result are optimized together to generate the edited sounds while preserving the remaining factors in the original audio. From the results analysis, we figured out that finding proper interpolation intervals and fine-tuning the diffusion model play important role in our task. We know that detailed audio captions must be trained in order to capture the temporal and spatial semantics from prompts, however, the size of the model and dataset are too large to train on time with our resources. Thus, our further work may focus on training detailed audio captions properly, so that the model can also generate the complex prompts while enhancing the fidelity of input audio and identity preservation.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/projects/'>projects</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/audio-text/" class="post-tag no-text-decoration" >audio-text</a> <a href="/tags/generation/" class="post-tag no-text-decoration" >generation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.linkedin.com/in/yoon-jin-chung-03783016a/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/blog/%EC%88%9C%EC%B0%A8%EC%A0%81-%EC%A0%95%EB%B3%B4%EB%A5%BC-%EB%8B%A4%EB%A3%A8%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EB%93%A4/">순차적 데이터를 다루는 딥러닝 모델들</a><li><a href="/blog/AIKorea-2019-%EB%A6%AC%EB%B7%B0/">AI Korea 2019 리뷰</a><li><a href="/blog/2019-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%9C%88%ED%84%B0%EC%BD%94%EB%94%A9-%EB%AC%B8%EC%A0%9C%ED%92%80%EC%9D%B4/">2019 프로그래머스 윈터 코딩 문제 풀이</a><li><a href="/blog/Audio-Based-Album-Cover-Generator/">Audio-based Album Cover Generator</a><li><a href="/blog/Music-Auto-Tagging/">Music Auto Tagging</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/generation/">generation</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/audio-text/">audio-text</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/dsp/">DSP</a> <a class="post-tag" href="/tags/metric-learning/">metric-learning</a> <a class="post-tag" href="/tags/models/">models</a> <a class="post-tag" href="/tags/music-image/">music-image</a> <a class="post-tag" href="/tags/music-tag/">music-tag</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc"></nav></div><script src="https://cdn.jsdelivr.net/npm/tocbot@4.20.1/dist/tocbot.min.js"></script></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/blog/Audio-Based-Album-Cover-Generator/"><div class="card-body"> <em class="small" data-ts="1611446400" data-df="ll" > Jan 24, 2021 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Audio-based Album Cover Generator</h3><div class="text-muted small"><p> This project is based on 2021 Fall GCT634 at KAIST github link: https://github.com/YoonjinXD/Audio_Based_Album_Cover_Generator Making album cover images is a difficult task for many amateur mus...</p></div></div></a></div><div class="card"> <a href="/blog/%EC%88%9C%EC%B0%A8%EC%A0%81-%EC%A0%95%EB%B3%B4%EB%A5%BC-%EB%8B%A4%EB%A3%A8%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EB%93%A4/"><div class="card-body"> <em class="small" data-ts="1564444800" data-df="ll" > Jul 30, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>순차적 데이터를 다루는 딥러닝 모델들</h3><div class="text-muted small"><p> Recurrent Neural Networks, 줄여서 RNN의 대표 모델들 공부. Introduction RNN은 기본적으로 순차적 정보를 가지는 sequential data(ex. time-series data) 를 효과적으로 학습하기 위해 고안된 모델이다. 데이터의 종류를 구분하는 방법은 여러가지가 있지만, 아주 간단하게는 many OR ...</p></div></div></a></div><div class="card"> <a href="/blog/AIKorea-2019-%EB%A6%AC%EB%B7%B0/"><div class="card-body"> <em class="small" data-ts="1564617600" data-df="ll" > Aug 1, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>AI Korea 2019 리뷰</h3><div class="text-muted small"><p> AI Korea 2019 리뷰 (공유용) 1. [산업체 세션2] SKT T-Brain, 김지원(SKT) 현재 SKT T-Brain 부서에서 진행해왔던 그리고 진행하고 있는 프로젝트들에 대해 소개하고 관련 이슈를 제시하는 세션이었습니다. SKT T-Brain 팀은 크게 4가지 이슈로 나뉘어 프로젝트를 진행하고 있습니다. Meta AI‘AI...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/blog/Vocoder-Project/" class="btn btn-outline-primary" prompt="Older"><p>Mimicking Vocal Tracks by Implementing and Applying Vocoder Effect</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/generation/">generation</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/audio-text/">audio-text</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/dsp/">DSP</a> <a class="post-tag" href="/tags/metric-learning/">metric-learning</a> <a class="post-tag" href="/tags/models/">models</a> <a class="post-tag" href="/tags/music-image/">music-image</a> <a class="post-tag" href="/tags/music-tag/">music-tag</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/yoonjinxd">Yoonjin Chung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0">Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-WC36HCMDDJ"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-WC36HCMDDJ'); }); </script>
