[ { "title": "Text-conditioned Audio Editing", "url": "/blog/Text-conditioned-Audio-Editing/", "categories": "projects", "tags": "AI, audio-text, generation", "date": "2023-03-02 00:00:00 +0000", "snippet": "2022 Fall AI605 Final Project, Yoonjin Chung, Junwon Leegithub link: https://github.com/YoonjinXD/Text-conditioned-Audio-Editingreport: https://github.com/YoonjinXD/Text-conditioned-Audio-Editing/b...", "content": "2022 Fall AI605 Final Project, Yoonjin Chung, Junwon Leegithub link: https://github.com/YoonjinXD/Text-conditioned-Audio-Editingreport: https://github.com/YoonjinXD/Text-conditioned-Audio-Editing/blob/master/AI605_Capstone_Report.pdfIntroductionSince finding specific data that satisfy various conditions in the real world, generating new data by editing samples has been a goal to achieve and a challenging task as well. In this study, we address the text-conditioned audio editing problem for the first time, to the best of our knowledge, by fine-tuning and interpolating text embeddings of audio captions. Our task aims to generate target sounds that are slightly modified from existing audio referring to given caption conditions. We address the text-conditioned audio editing task, which has rarely been studied. We optimize the given text embedding and fine-tune the discrete diffusion model to generate semantically edited sound. We provide the qualitative analysis on caption prompts and interpolation intensity by comparing our results from various types of prompts.Related WorksDiffsound(Yang et al., 2022) is the first model that solves the text-to-audio generation task directly. The proposed a discrete-diffusion-based audio generative model conditioned on a text prompt. The model consists of a pretrained CLIP text encoder, a discrete diffusion model, a Vector Quantized Variational Autoencoder(VQ-VAE) and a vocoder. After the text prompt is encoded as a feature, the diffusion decoder transfers the text feature to a sequence of quantized tokens which is passed to the pretrained VQ-VAE decoder to reconstruct a mel-spectrogram. Finally, the vocoder generates a waveform from the transferred mel-spectrogram.Imagic(Kawar et al., 2022) is the only model, that tackles the image editing task conditioned on a text prompt. Given a single target text prompt and image pair, the model is fine-tuned in several steps. As the given prompt does not describe the original image but the change to be applied, only fine-tune the text encoder first on text-image pair while freezing others, to get optimized embedding which is distinct from the target embedding acquired from original text encoder in Imagen.To get the edited image, they finally input the embedding which is a linear interpolation between the target and optimized embeddings. The authors showed the interpolation is semantically valid in a qualitative manner by providing examples corresponding to several editing categories.Proposed MethodArchitectureWe exploit pretrained Diffsound model as our base framework as it is the only text-to-audio model which is open-source. As we finetune only some part of Diffsound initializing it with provided pretrained weights, the overall architecture is identical. See Fig 1 to see a simply summarized diagram.Among the components, we do not fintune VQ-VAE, the decoder part that generates mel spectrogram from a sequence of quantized mel spectrogram tokens, and MelGAN, the vocoder, which is both trained on Audioset respectively. The two are not involved in processing text caption but in generating the audio sound. In this paper, we focus on CLIP which encodes the text prompt, and discrete diffusion model which generates a quantized token sequence conditioned on the text embedding.MethodsWe follow the proposed method suggested in Imagic (Kawar et al., 2022) to achieve our goal. Given the original input audio, we aim to output the edited audio through the semantics included in the text prompt. Note that the prompt is not a caption that describes the original audio but the desirable modified version of it. Let the text representation which encodes this prompt be a target embedding. As there is no way for the text-to-audio model to generate audio similar to the original one, the key idea is to find an optimized embedding that enables it when passed through the generative layers.The brief procedure is the following: We finetune CLIP text embedding layer to acquire an optimized embedding vector near the target text embedding. The ideal optimized text embedding, as an input of the audio generative model, may nearly reconstruct the original audio. We finetune the discrete diffusion model, with optimized embedding representation as input, to generate the original audio better. Compute the linear interpolation of the target embedding and the optimized embedding and input to the generation part. Proper interpolation enables to find a representation that maintains balance between audio fidelity and text prompt alignment. which is described visually in Fig 2.ResultWe have verified that substitution in sound source (i.e. who/what is making sound, which kind of sound has occurred) is successful while preserving the structural feature of the audio.Sample 1(Original)A car horn sounds loudly and then fades away → (Edited)A bell sounds loudly and then fades away Original Editied Sample 2(Original)A woman gives a speech → (Edited)A man gives a speech Original Editied As you can see from the given samples, it is able to edit the horn to the bell sound while maintaining a structure in which the bell rings once loudly and then fades away as same as the original audio.LimitationsSome edits are applied very subtly, so that they do not align well with the target text. Since the audio generative diffusion model that we used is only trained with ontological tags from Audioset, it seems that cannot capture complex semantics such as temporal information(e.g. after, then) or spatial information(e.g. foreground, background) in the audio domain.Conclusion &amp; Future WorkIn this study, we propose the first text-conditioned audio editing method based on the pre-trained generative diffusion model. With our method, an existing audio and caption pair describing the desired editing result are optimized together to generate the edited sounds while preserving the remaining factors in the original audio. From the results analysis, we figured out that finding proper interpolation intervals and fine-tuning the diffusion model play important role in our task. We know that detailed audio captions must be trained in order to capture the temporal and spatial semantics from prompts, however, the size of the model and dataset are too large to train on time with our resources. Thus, our further work may focus on training detailed audio captions properly, so that the model can also generate the complex prompts while enhancing the fidelity of input audio and identity preservation." }, { "title": "Mimicking Vocal Tracks by Implementing and Applying Vocoder Effect", "url": "/blog/Vocoder-Project/", "categories": "projects", "tags": "DSP", "date": "2022-06-27 00:00:00 +0000", "snippet": "Spring 2022 GCT535 Final Project, Yoonjin Chung, Junwon LeeDemo page: https://yoonjinxd.github.io/vocoder_demoDaftpunk, Zedd, Stevie Wonder와 같은 보컬 트랙에 등장하는 보코더 효과를 파이썬으로 재현해보자! band-pass 필터와 RMS 필...", "content": "Spring 2022 GCT535 Final Project, Yoonjin Chung, Junwon LeeDemo page: https://yoonjinxd.github.io/vocoder_demoDaftpunk, Zedd, Stevie Wonder와 같은 보컬 트랙에 등장하는 보코더 효과를 파이썬으로 재현해보자! band-pass 필터와 RMS 필터를 통해 클래식한 channel vocoder를 구현 조절 가능한 매개변수들을 구현하고 비교(e.g. F0, Number of Frequency Bands, Frequency Scale, Random Noising, Formant Shifting and Beta between modulator and carrier) 결과 소리를 개선하기 위해 compressor와 expander 구현이렇게 구현한 수제(?) 보코더와 매개변수를 사용하여 타겟 아티스트들을 따라해보고, 또 다양한 유형의 carrier를 적용해보며 이것저것 흥미로운 결과들을 만들어보았다.재밌었던 플젝이지만… ㅎㅎ 역시 보코더는 밖에서 사드세요!Figure.1 Implementation of VocoderFigure.2 Comparison between original STFT and vocoded STFT" }, { "title": "Music Auto Tagging", "url": "/blog/Music-Auto-Tagging/", "categories": "projects", "tags": "AI, music-tag, classification, metric-learning", "date": "2021-11-22 00:00:00 +0000", "snippet": "This project is based on Fall 2021 GCT 634 at KAISTgithub link: https://github.com/YoonjinXD/Music-Auto-TaggingSUMMARY I implemented and performed related experiments to verify the improvement sug...", "content": "This project is based on Fall 2021 GCT 634 at KAISTgithub link: https://github.com/YoonjinXD/Music-Auto-TaggingSUMMARY I implemented and performed related experiments to verify the improvement suggestion for the multi-label classification. I implemented music-tag pair triplet dataset and sampling for music retrieval by tag. I applied data augmentation, multi-channel mel-spectograms and practical searching for hyperparameters as common strategies to improve the models. To improve music tag classification, I suggested the CNN architecture for music data and applied focal loss function. In retrieval problem, I mainly focused on building a triplet data sampling strategy and suggested classification-based triplet sampling.Through the suggested approaches, the final improved models result in 0.86% roc accuracy for classification and ({‘R@1’: 0.47634560692813116, ‘R@2’: 0.6166209464753155, ‘R@4’: 0.7485045429220182, ‘R@8’: 0.8455286622762342}) recall for retrieval.You can check more details in the report here. The approaches and detailed methods for the improvement are explained in section 2. Remained discussion and future study are summarized in section 3.You can also check all the implementations in the uploaded notebook." }, { "title": "Audio-based Album Cover Generator", "url": "/blog/Audio-Based-Album-Cover-Generator/", "categories": "projects", "tags": "AI, music-image, generation", "date": "2021-01-24 00:00:00 +0000", "snippet": "This project is based on 2021 Fall GCT634 at KAISTgithub link: https://github.com/YoonjinXD/Audio_Based_Album_Cover_GeneratorMaking album cover images is a difficult task for many amateur musicians...", "content": "This project is based on 2021 Fall GCT634 at KAISTgithub link: https://github.com/YoonjinXD/Audio_Based_Album_Cover_GeneratorMaking album cover images is a difficult task for many amateur musicians who want to share their own works online. Random image generation could create certain images; however, it is not enough to represent the artists’ music. Our goal is to generate a plausible image that represents the feature of an album’s music. In this paper, we proposed an album cover image generation model considering the belonging tracks’ audio features. Our model is consisting of three different modules, VQ-VAE which learn album cover image distribution, pre-trained ShortChunkCNN which extracts track audio’s features embedding and the mapping layers the audio embedding to the latent space of album images, Audio2ImageNet, we named as. We obtained generated images by training the whole model except for the pre-trained audio feature extractor. Then the result samples and dataset are analyzed in detail, figuring out the relation between album images and audio tracks." }, { "title": "2019 프로그래머스 윈터 코딩 문제 풀이", "url": "/blog/2019-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%9C%88%ED%84%B0%EC%BD%94%EB%94%A9-%EB%AC%B8%EC%A0%9C%ED%92%80%EC%9D%B4/", "categories": "reviews", "tags": "algorithm", "date": "2019-11-16 00:00:00 +0000", "snippet": "2019년 프로그래머스 동계 인턴쉽 코딩테스트. 총 3문제로 난이도는 3번을 제외하고 무난했고, 무엇보다 문제 자체가 정말 재미있었어서 정리. 문제들은 프로그래머스 웹사이트에 공개 되어있다.문제 풀 수 있는 곳 - 클릭 멀쩡한 사각형 문제 설명 가로 길이가 Wcm, 세로 길이가 Hcm인 직사각형 종이...", "content": "2019년 프로그래머스 동계 인턴쉽 코딩테스트. 총 3문제로 난이도는 3번을 제외하고 무난했고, 무엇보다 문제 자체가 정말 재미있었어서 정리. 문제들은 프로그래머스 웹사이트에 공개 되어있다.문제 풀 수 있는 곳 - 클릭 멀쩡한 사각형 문제 설명 가로 길이가 Wcm, 세로 길이가 Hcm인 직사각형 종이가 있습니다. 종이에는 가로, 세로 방향과 평행하게 격자 형태로 선이 그어져 있으며, 모든 격자칸은 1cm x 1cm 크기입니다. 이 종이를 격자 선을 따라 1cm × 1cm의 정사각형으로 잘라 사용할 예정이었는데, 누군가가 이 종이를 대각선 꼭지점 2개를 잇는 방향으로 잘라 놓았습니다. 그러므로 현재 직사각형 종이는 크기가 같은 직각삼각형 2개로 나누어진 상태입니다. 새로운 종이를 구할 수 없는 상태이기 때문에, 이 종이에서 원래 종이의 가로, 세로 방향과 평행하게 1cm × 1cm로 잘라 사용할 수 있는 만큼만 사용하기로 하였습니다.가로의 길이 W와 세로의 길이 H가 주어질 때, 사용할 수 있는 정사각형의 개수를 구하는 solution 함수를 완성해 주세요. 제한 사항 W, H : 1억 이하의 자연수 입출력 예 W H result 8 12 80 풀이 처음 몇분동안은 노가다로 규칙을 구해보려다 답답해서 구글링을 해보니 생각보다 쉽게 공식을 찾을 수 있었다. 이 블로그를 참고했다. 대각선에 의해 잘려지는 사각형의 개수는 대각선과 만나는 격자점의 개수와 관련이 있고, 공식에 따르면 (가로) + (세로) - (가로 세로 최대 공약수) 개의 사각형이 대각선 아래에 놓이게 된다. 요 공식 덕분에 빠르게 접근할 수 있었다. 유클리드 호제법으로 공약수 구하는 건 유명하니, 금방 짤 수 있다. 코드 def gcd(x, y): if y == 0: return x return gcd (y, x%y) def solution(w,h): if w &lt; h: w, h = h, w return (w*h) - (w+h-gcd(w, h)) 종이 접기 문제 설명 직사각형 종이를 n번 접으려고 합니다. 이때, 항상 오른쪽 절반을 왼쪽으로 접어 나갑니다. 다음은 n = 2인 경우의 예시입니다. 먼저 오른쪽 절반을 왼쪽으로 접습니다. 다시 오른쪽 절반을 왼쪽으로 접습니다. 종이를 모두 접은 후에는 종이를 전부 펼칩니다. 종이를 펼칠 때는 종이를 접은 방법의 역순으로 펼쳐서 처음 놓여있던 때와 같은 상태가 되도록 합니다. 위와 같이 두 번 접은 후 종이를 펼치면 아래 그림과 같이 종이에 접은 흔적이 생기게 됩니다. 위 그림에서 ∨ 모양이 생긴 부분은 점선(0)으로, ∧ 모양이 생긴 부분은 실선(1)으로 표시했습니다. 종이를 접은 횟수 n이 매개변수로 주어질 때, 종이를 절반씩 n번 접은 후 모두 펼쳤을 때 생기는 접힌 부분의 모양을 배열에 담아 return 하도록 solution 함수를 완성해주세요. 제한 사항 종이를 접는 횟수 n은 1 이상 20 이하의 자연수입니다. 종이를 접었다 편 후 생긴 굴곡이 ∨ 모양이면 0, ∧ 모양이면 1로 나타냅니다 가장 왼쪽의 굴곡 모양부터 순서대로 배열에 담아 return 해주세요. 입출력 예 n result 1 [0] 2 [0,0,1] 3 [0,0,1,0,0,1,1] 풀이 이것 역시 구글링을 통해 힌트를 얻었다. 이 사이트 를 참고했다. 잘 보면 처음 접은 가운데 ‘0’ 을 기준으로 양쪽이 대칭이다. 예컨대 n이 3일 때를 보면, 가운데 ‘0’을 기준으로 오른쪽 세개의 굴곡 ‘0 0 1’에서 0은 1로 1은 0으로 뒤바꾼 후 순서를 거꾸로 배열하면 ‘ 0 1 1’이 된다. 처음 접은 굴곡인 0을 기준으로 그 다음 접을 때부터는 완전히 대칭으로 굴곡이 생겨나며, 이렇게 함께 접힌 굴곡은 한쪽이 0이면 다른 한쪽은 1일 수밖에 없어서 그렇다. n이 2일 때 어떻게 양쪽에 0과 1이 추가되었는 지 생각해보고, 이후 n이 하나씩 늘어날수록 Top-down 재귀처럼 똑같은 일이 반복됨을 알 수 있다. 그러므로 가운데 0을 기준으로 왼쪽이나 오른쪽 중 한 쪽의 굴곡들만 파악하면 된다.그런데 한쪽의 굴곡은 어떻게 파악해야 할까? 놀랍게도 n-1의 결과가 n의 왼쪽 굴곡이 된다 (n=2일 때의 결과가 n=3의 왼편 굴곡과 같음). 다 구했다. 이제 코딩만 하면 된다. 마침 0과 1이니 비트 연산자를 사용했다.개인적으로 이 문제는 풀면서도 진짜 재미있었다. 종이 접기에 이런 규칙이…! 코드 def solution(n): l_side = [0] for _ in range(n-1): l_side = l_side + [0] + [ele^1 for ele in reversed(l_side)] return l_side 원래는 이것보다 2 ~3 줄 더 길었는데 다른 분들 풀이를 보고 불필요한 부분을 줄였다. 추가 궁금증 처음 제출을 했을 때 테스트 케이스 중 유독 시간이 오래 걸리는 게 몇개 있었다. 왜 오래 걸리지 싶어 살펴보다가, 리스트를 뒤집을 때 reversed() 함수를 사용하니 시간이 확 단축 된다는 것을 발견했다 (처음 제출할 땐 reversed 대신 [::-1] 로 뒤집었기 때문에 오래 걸렸다). 실제로 시간을 재어보니, import time lst = [n for n in range(10000)]start1 = time.time()lst = lst[::-1]t1 = time.time() - start1print('t1 = ', t1*10000) start2 = time.time()lst = reversed(lst)t2 = time.time() - start2print('t2 = ', t2*10000) t1 = 1.373291015625t2 = 0.021457672119140625 reversed() 가 [::-1] 보다 훨씬 빠르다. 왜지…? 지형 이동 문제 설명 N x N 크기인 정사각 격자 형태의 지형이 있습니다. 각 격자 칸은 1 x 1 크기이며, 숫자가 하나씩 적혀있습니다. 격자 칸에 적힌 숫자는 그 칸의 높이를 나타냅니다. 이 지형의 아무 칸에서나 출발해 모든 칸을 방문하는 탐험을 떠나려 합니다. 칸을 이동할 때는 상, 하, 좌, 우로 한 칸씩 이동할 수 있는데, 현재 칸과 이동하려는 칸의 높이 차가 height 이하여야 합니다. 높이 차가 height 보다 많이 나는 경우에는 사다리를 설치해서 이동할 수 있습니다. 이때, 사다리를 설치하는데 두 격자 칸의 높이차만큼 비용이 듭니다. 따라서, 최대한 적은 비용이 들도록 사다리를 설치해서 모든 칸으로 이동 가능하도록 해야 합니다. 설치할 수 있는 사다리 개수에 제한은 없으며, 설치한 사다리는 철거하지 않습니다. 각 격자칸의 높이가 담긴 2차원 배열 land와 이동 가능한 최대 높이차 height가 매개변수로 주어질 때, 모든 칸을 방문하기 위해 필요한 사다리 설치 비용의 최솟값을 return 하도록 solution 함수를 완성해주세요. 제한 사항 land는 N x N크기인 2차원 배열입니다. land의 최소 크기는 4 x 4, 최대 크기는 300 x 300입니다. land의 원소는 각 격자 칸의 높이를 나타냅니다. 격자 칸의 높이는 1 이상 10,000 이하인 자연수입니다. height는 1 이상 10,000 이하인 자연수입니다. 입출력 예 land height result [[1, 4, 8, 10], [5, 5, 5, 5], [10, 10, 10, 10], [10, 10, 10, 20]] 3 15 [[10, 11, 10, 11], [2, 21, 20, 10], [1, 20, 21, 11], [2, 1, 2, 1]] 1 18 풀이 마지막 문제… 좀 어렵다. BFS나 DFS로 사다리 없이 이동 가능한 그룹들을 나눠야 한다는 것까지는 알겠는데 그 이후가 문제였다. 그룹을 나눈 뒤 최소 비용을 어떻게 구하나- 다익스트라? 재귀? 갈피를 못 잡다가 테스트 시간이 얼마 안 남아서 일단 그룹 나누기에 착수했다. BFS를 사용했고 노드 클래스를 만들어 해당 칸의 높이와 속해있는 그룹을 식별할 번호를 부여했다. 아직 아무데에도 속하지 않은(그룹 번호가 0) 노드들에 대해 bfs를 돌려 그룹까지는 잘 구분한 것 같았다. 이제 각 그룹 간 이동가능한 최소 사다리 높이를 구해놓고 그 높이들을 잘 조합하여 전체 최소 비용을 구하면 된다고 생각하고 몇 줄 적기 시작했는데 테스트 시간이 끝났다.그리고 몇 주 후 이 사이트 에서 공식 해법을 공개했다. 최소 사다리 높이를 조합할 방법을 찾지 못했었는데 최소 신장 트리(Minimum Spanning Tree) 를 쓰면 된다고 한다. 근데 정작 풀이 코드는 주지 않아서 일단 시키는대로 한번 짜보았다. 나눈 그룹 번호를 참고하여 각 그룹별 생성 가능한 간선들을 생성한 뒤, 최소 신장 트리 중에서도 크루스칼 알고리즘을 구현하였다. Union, FindSet과 같은 기본 기능을 구현한 후 수도 코드 그대로 따라 작성하였다. 자세한 건 이 블로그 를 참고하였다. 이렇게 했는데 30개 중 마지막 테스트 케이스 10개에서 시간 초과가 떴다. 20개라도 통과한 걸 위안 삼으며 어떻게 시간초과를 해결할 지 고민해야겠다. 코드(더러움 주의) from collections import deque class Node(): def __init__(self, h): self.h = h self.group = 0 def bfs(land, x, y, height, group_num): group_lst = [] n = len(land) dx = [1, -1, 0, 0] dy = [0, 0, 1, -1] que = deque() que.append((x, y)) while (que): x, y = que.popleft() group_lst.append(land[x][y].h) land[x][y].group = group_num for i in range(4): next_x, next_y = x + dx[i], y + dy[i] if 0 &lt;= next_x &lt; n and 0 &lt;= next_y &lt; n: diff = abs(land[next_x][next_y].h - land[x][y].h) if land[next_x][next_y].group == 0 and diff &lt;= height: que.append((next_x, next_y)) return land, group_lst def findRoot(x, parent): if x == parent[x]: return x return findRoot(parent[x], parent) def union(x, y, parent, level): x = findRoot(x, parent) y = findRoot(y, parent) if x == y: return if level[x] &gt; level[y]: x, y = y, x #항상 x가 더 작은 트리가 되도록 설정 parent[x] = y if level[x] == level[y]: level[x] += 1 #깊이가 같을 경우 x의 level을 늘려준다. def MST_kruskal(edges, group_cnt): # SORTED edges = (distance, (A, B)) parent = [i for i in range(0, group_cnt)] level = [1 for _ in range(0, group_cnt)] cost = 0 for edge, (a, b) in edges: if findRoot(a, parent) != findRoot(b, parent): cost += edge union(a, b, parent, level) return cost def solution(land, height): answer = 0 n = len(land) for row_idx, row in enumerate(land): land[row_idx] = list(map(Node, row)) # 1. Group movable stages group_cnt = 1 group_dict = [] for i in range(n): for j in range(n): if land[i][j].group == 0: land, temp = bfs(land, i, j, height, group_cnt) group_dict.append(temp) group_cnt += 1 # 2. Create edges dx = [1, -1, 0, 0] dy = [0, 0, 1, -1] edges = [] # (distance, (A, B)) for x in range(n): for y in range(n): for i in range(4): next_x, next_y = x + dx[i], y + dy[i] if 0 &lt;= next_x &lt; n and 0 &lt;= next_y &lt; n: if land[next_x][next_y].group != land[x][y].group: diff = abs(land[next_x][next_y].h - land[x][y].h) a = land[x][y].group b = land[next_x][next_y].group if a &gt; b: a, b = b, a edges.append((diff, (a, b))) # 3. MST algorithm if edges: edges = [ele for ele in set(edges)] edges = sorted(edges, key=lambda x: x[0]) return MST_kruskal(edges, group_cnt) " }, { "title": "AI Korea 2019 리뷰", "url": "/blog/AIKorea-2019-%EB%A6%AC%EB%B7%B0/", "categories": "reviews", "tags": "AI, seminar", "date": "2019-08-01 00:00:00 +0000", "snippet": "AI Korea 2019 리뷰 (공유용)1. [산업체 세션2] SKT T-Brain, 김지원(SKT)현재 SKT T-Brain 부서에서 진행해왔던 그리고 진행하고 있는 프로젝트들에 대해 소개하고 관련 이슈를 제시하는 세션이었습니다. SKT T-Brain 팀은 크게 4가지 이슈로 나뉘어 프로젝트를 진행하고 있습니다. Meta AI‘AI가 AI를 만든다...", "content": "AI Korea 2019 리뷰 (공유용)1. [산업체 세션2] SKT T-Brain, 김지원(SKT)현재 SKT T-Brain 부서에서 진행해왔던 그리고 진행하고 있는 프로젝트들에 대해 소개하고 관련 이슈를 제시하는 세션이었습니다. SKT T-Brain 팀은 크게 4가지 이슈로 나뉘어 프로젝트를 진행하고 있습니다. Meta AI‘AI가 AI를 만든다’ 라는 슬로건을 걸고 딥러닝 모델 학습 자동화 플랫폼 시스템을 구축하는 팀으로, Meta AI 인터페이스, 핵심 알고리즘 개발, 클러스터 인프라 구축 등으로 나뉘어 개발이 이루어지고 있습니다. 이 팀은 주어진 데이터를 학습하는 데 가장 효율적인 Neural Net과 그에 맞는 hyperparameter들을 자동적으로 탐색하여, 전문가나 사용자의 개입을 최소화함과 동시에 가장 적합한 AI 솔루션을 제공함을 주 목적으로 두고 있습니다. 현재 SKT T-brain 팀에서 가장 활발하게 개발되고 있다고 합니다. Conversational AI자연어처리와 관련된 연구개발 팀으로, Question Answering, Dialogue Management 등의 연구를 통해 대화형 인공지능 개발(ex. NUGU) 에 주력하고 있습니다. Visual AI컴퓨터비전과 관련된 연구개발 팀으로, Visual Question Answering, Visual Turing Test, Visual Relationship Detection 연구 등을 주력으로 하며 특히 자율주행 관련 테스크에 주목하고 있다고 합니다. Music AIAI 스피커 ‘네모’를 통해 활용하려 하는 상용화 서비스로서 Singing Voice &amp; Instrument Performance Translation 을 주로 연구하는데, 이 기술은 입력된 음악을 사용자가 원하는 목소리 및 악기 연주 스타일로 자동 변환해주는 기술입니다. 지난 프로젝트들 DiscoGAN기존의 GAN(Generative Adversarial Network) 모델들(DCGANs, ImprovedGan, cGan, pix2pix 등등)은 학습할 때 Paired Dataset, 즉 쌍이 매칭된 데이터셋이 필요하다는 한계점을 가지고 있었습니다. 쌍이 이루어진 데이터셋으로 학습을 하면 좋겠지만, 실제 세계에서 이러한 데이터를 찾기란 쉽지 않습니다. 이렇게 Supervised로서 제안된 모델을 Unsupervised 모델로 발전시켰다는 점에서 DiscoGAN(DiscoveryGAN)은 큰 의미를 가집니다. DiscoGAN은 기존 방식과 같이 Generator가 생성해낸 이미지를 Discriminator가 진위 여부를 판단하되, 해당 이미지를 다시 원래의 도메인으로 변환하는 또 다른 Generator를 배치하여 처음 인풋 이미지와 비교하는 Back Translation 기법을 사용합니다. 다시 말해, 인풋과 아웃풋간의 페어링이 되어 있지 않으니, 가짜 이미지 생성시 실제 인풋 이미지가 너무 많이 바뀌어 버리지 않도록 제재를 가하는 것이라 할 수 있습니다. 이 기법을 적용함으로써, DiscoGAN의 loss값은 Discrimination Loss(진위 여부 판별 loss) + Consistency Loss(다시 되돌아 온 생성된 이미지와 인풋 이미지 사이의 loss) 로 계산됩니다.이 논문은 CycleGAN이라는 유명한 모델과 매우 유사한 아이디어를 제안하는 논문으로, CycleGAN보다 조금 일찍 발표하여 당시 전세계의 이목을 끌었다고 합니다. (그런데 CycleGAN 때문에 묻혔다…?) 기존 GAN의 한계점을 극복하고 특히 Cross-domain 영역에서 좋은 성과를 냈기에 유의미한 논문이라고 생각합니다. [논문], [코드], CycleGAN 네이버D2 강연 Continual learningContinual Learning은 여러 task를 하나의 모델에 순차적으로 학습시켜, 최종적으로 모든 task의 수행이 가능한 모델을 학습시키는 기술입니다. 하지만 새로운 task를 학습 시킬 때, 모델이 새 task를 풀도록 완전히 바뀌어 이전 task를 모두 잊어버리는 Catastrophic Forgetting 현상이 발생하는데요, 이를 해결하기 위해 연구 팀은 GAN 기반 Generator와 주어진 task를 해결하는 solver의 두 모델로 구성된 Deep Generative Replay Framework를 제안하였습니다. Generator는 이전 모델의 Generator에서 생성된 가짜 데이터와 현재 task의 진짜 데이터를 합친 분포함수를 재현하도록 학습되고, 반면 solver는 그에 대한 적절한 타깃을 예측하도록 학습되는 구조입니다. 해당 연구는 ‘Continual Learning with Deep Generative Replay’ 라는 제목으로 논문을 발표하여 지난 2017년 NIPS에 등재되었다고 합니다. [논문], [코드] 현재 진행중인 프로젝트들 SKT T-brain팀은 Automatical Meta learning, 즉 자동적으로 메타 데이터를 학습할 수 있는 딥러닝 모델을 구축하고 hyperparmeter들까지 탐색해주는, 말 그대로 End-to-End AI 자동화 서비스 플랫폼을 개발하고 있습니다. 자동화된 도구로서 역할하는 것을 목표로, 다양한 딥러닝 관련 프로젝트에서 활용될 수 있으며 비전문가도 손쉽게 사용할 수 있도록 상용화할 계획이라고 합니다. Progressive NasNet에서 영감을 받아 착수한 프로젝트로 AutoML과 유사한 플랫폼이라고 볼 수 있습니다. 자연어처리 분야에서는 BERT를 한국어에 조금 더 효과적으로 적용하기 위한 koBERT 프로젝트와 SUMBT(Slot-Utterance Matching for Universal and Scalable Belief Tracking) 프로젝트 등이 있습니다. SUMBT 는 올해 7월에 논문으로 제출되었다고 합니다. 관련 설명을 조금 해주셨는데, 지식이 짧아 이해하진 못했습니다. 시간이 되면 논문을 한번 읽어보고 싶습니다. [논문] T-Brain 에서 연구하고 있는 주제들 전부 정말 흥미진진 했습니다. 특히, Auto Meta와 같은 자동 모델 구축 및 최적화 알고리즘이 정말 필요한 일 일까 궁금하기도 합니다. NasNet을 강하게 비판했던 논문(Exploring Randomly Wired Neural Networks for Image Recognition)이 기억에 남는데, 여기서 주장하는 이야기가 아예 헛소리는 아니지 않을까…? 라는 생각이 들었습니다. 다른 분들은 어떻게 생각하는지 궁금하고 주제에 대해 이야기를 나누어보고 싶습니다.2. [패턴인식/기계학습 여름학교 3] 최적화를 위한 반복법, 김동환(KAIST)딥러닝의 최적화 메서드들에 대한 수학적 접근들을 주로 다루었던 세션입니다. Optimization먼저, 최적화 문제란 어떤 목적함수의 함수값을 최적화(최대화 or 최소화)시키는 파라미터 조합을 탐색하는 문제를 말합니다. 목적 함수가 모든 파라미터에 대해 일차 이하의 다항식으로 구성되면 선형 최적화(linear optimization) 문제라 하고 그 외의 경우는 비선형 최적화(nonlinear optimization)이라 부릅니다. Iterative Method(반복법)이러한 최적화 문제를 푸는 원리는, 현재 위치에서 함수값이 감소하는 방향으로(최소화 문제라고 가정) 조금씩 파라미터 값을 이동해 나가는 것입니다. 이러한 이동을 더 이상 감소할 수 없는 곳(global minima)에 다다를 때까지 반복합니다. 이때 중요한 이슈는 어느 방향으로 갈 것인지, 얼만큼 이동할 것인지 결정하는 것입니다. 이 결정의 차이에 따라 다양한 최적화 기법이 나뉘어집니다. Newton method Gradient method SGD ADAM Convergence Rate(수렴속도)수렴 속도는 주어진 시간 내에 얼마나 빨리 원하는 해에 접근할 수 있는가에 대한 계산입니다. 조금 더 넓게 이야기하자면, 어떤 주제에 대한 적절한 답을 찾는데 어떤 알고리즘이 가장 빠르게 도달할 수 있는가에 대한 문제입니다. Aysmptotic Non-asymtotic * Sublinear / Linear / Superlinear rate Gradient Method 사실 ADAM paper에서 Non-convex에서의 최적화 함수의 수렴 증명을 하지 못했습니다. 대신 Convex 스페이스에서 수렴함을 보이고 nonconvex 역시 수렴할 것이라는 믿음을 바탕으로 최적화 기법을 적용해보았는데 잘 되더라- 로 마무리 지었습니다. Convex optimization problems에 대한 증명은 여기 있습니다. Non-convex 상황에서는 convex와는 달리 global minima의 존재 여부가 확실치 않고 여러개의 global minima가 나올 수 있기 때문에 그 존재를 정의하거나 탐색하기 매우 어렵습니다. 관련 연구 및 증명을 몇가지 설명해주셨는데… 어려웠습니다. Non-convex optimization problem에 대해 공부가 필요할 것 같습니다. Smooth but nonconvexlocal minima의 존재에 의해 기존에 정의된 최적화 수식들이 의미가 없어질 뿐더러, Initial condition에 따라 다루려는 범위가 달라진다는 복잡한 문제들이 있습니다. Momentum method이런 문제들을 해소하고자 제안된 것이 Momentum method 입니다. First-order method: 이전 누적된 gradient를 현재 계산에 사용하여 계산량을 늘리지 않고도 좋은 결과를 얻자는 아이디어. 일종의 관성 또는 가속도처럼 생각할 수 있다. Heavy-Ball method: 위와 같은 아이디어를 살짝 개선시킨 방법입니다. ADAM은 요 수식의 알파베타를 matrix로 변환한 후 이것을 non-convex 공간에 적용한 모델이다. Nesterov’s method: 현재 위치에서 이전 속도만큼 일단 전진해보고 gradient를 계산한다. 선험적으로 혹은 모험적으로 에러를 교정해나간다고 할 수 있다. OGM ? nonconvex에서 Gradient를 어떻게 더 줄일까 SGD는 local minima를 벗어난다? -&gt; 사실상 증명은 X -&gt; saddle point 연구로 이어짐 Saddle Point사실상 대부분의 딥러닝 모델들은 Non-convex 상황에서 학습되기 때문에 non-convex Gradient를 어떻게 더 줄일 수 있을까에 대한 연구는 꾸준히 진행되어 왔습니다. 특히, Global minima가 존재한다는 가정 하에 Non-convex 모델은 convex와 달리 local minima를 가질 수 있고, 그 local minima를 탈출해야 한다는 이론이 전반적으로 납득되어 왔었습니다. 하지만 2014년 이 문제에 대한 논문이 하나 발표되었는데, 이 논문은 기존 상식을 깨고 local minima 문제가 사실은 고차원 공간에서는 발생하기 매우 희귀한 경우라고 주장했습니다. 어떤 critical point가 local minima가 되기 위해서는 모든 축의 방향으로 함수 그래프가 아래로 볼록해야 하는데, 축(차원)이 많아질수록 그 확률이 희박해지기 때문입니다. 논문에서는 Non-convex 상황에서 설령 local minima가 발생한다 하더라도, 그는 global minima와 거의 유사한 수준의 값이며, 게다가 실제 세계의 데이터셋은 non-convex 스페이스에 존재하므로 차라리 multiple global minima라 불러야 한다고 말입니다. 또한 이전에 모멘텀 기법이 성과를 보였던 것은 local minima를 탈출해서가 아니라, saddle point(안정점)을 탈출하기 때문이라고 합니다. 즉, non-convex 스페이스에서의 critical point들의 대부분은 saddle point 라는 주장을 하고 있는 것입니다. 이들은 위 주장과 함께, saddle point 탈출과 minima 근처에서의 수렴 속도 제어에 조금 더 초첨을 맞춘 saddle-free-newton-method 라는 새로운 최적화 알고리즘을 제안하였습니다. 아직 이들의 주장이 명확하게 증명되거나 반증된 것은 아니기 때문에 어떤 것이 사실일 지는 모르겠으나 개인적으로 정말 흥미로웠습니다.이 세션을 듣고 말로만 듣던 Convex Optimization(볼록 최적화) 분야가 이런 거구나 알게 되었습니다. 한편으로는 딥러닝 기저에 깔려있는 아이디어들이 모두 이러한 수학적 증명 및 논리로부터 시작되었음을 느낄 수 있었고 생각보다 흥미로운 내용들이 많다는 것을 알게 되었습니다. 항상 당연하게만 생각하던 딥러닝 메서드들에 대해 왜? 라는 반문을 할 수 있었고, 그런 반문에서부터 breakthrough가 일어남을 배울 수도 있었습니다.이 세션은 개인적으로 가장 인상적인 세션이었습니다. 딥러닝 관련 공부를 하면서 답답한(?) 부분 중 하나가 바로 어떤 결과에 대한 명확한 논증이 어렵다는 점이 아닐까 싶은데요. 학습 과정을 설명하기 위한 시각화나 여러 추측 실험이 있긴 있는 것 같은데, 수학적 증명을 통한 명확한 해석을 찾아보기는 어려웠던 것 같습니다. 이 세션을 통해 볼록 최적화 문제에 대해 한번 공부해보고 싶다는 생각을 하게 되었습니다.(+) 최적화 공부 시작하기에 좋은 자료3. [패턴인식/기계학습 여름학교 5] 시계열 데이터와 기계학습 문제를 풀기 위한 신경망 구조, 정준영(Google DeepMind)시계열 데이터(time-series data) 문제를 풀 수 있는 딥러닝 모델들을 주로 다룬 Introduction 세션입니다. 고전적 솔루션인 Kalman, Hidden Markov model부터 기본 RNN과 Long-term dependency 문제, 그 문제를 해결하기 위해 제안됐었던 LSTM, GRU 등 전체 흐름을 짚어보며 각 모델의 주요 아이디어를 recap 하는 시간이었습니다. 특별한 주제를 다룬 세션은 아니었지만, 최근 많이 들리는 XLNet 에 대해 관심 있었기 때문에 후반부 내용이 흥미로웠습니다. 전반부(RNN ~ Transformer) 내용은 이전에 정리해둔 내용이 있으므로, 세션의 후반부(BERT, XLNet, Transformer XL)를 중심으로 리뷰하겠습니다. BERTBERT 모델을 한 마디로 설명하자면, Transformer의 Encoder 블럭만 가져다가 여러개 쌓은 모델이라고 할 수 있습니다. 논문은 두가지 버전을 함께 소개하는데, 기본 BERT 모델인 BERT_base 는 12개의 인코더를 이어붙였고, 좋은 결과 수치를 얻기 위해 만든 BERT_large 는 24개의 인코더를 연결해놓았습니다. 뿐만 아니라 원래 논문보다 더 높은 차원의 Feed-forward Net을 배치하고, attention head를 더 많이 만들었고, Input sequence에 대한 embedding을 개선(WordPiece Embedding)시켰으며, Masked LM이라는 새로운 unsupervised training 알고리즘을 통해 양방향 context 분석을 가능케 하며 다양한 NLP task들을 성공적으로 수행하였습니다. Masked LM은 한 문장에서 랜덤으로 선택한 단어들을 마스킹한 후(구멍 뽁뽁 뚫기), 모델이 주변 단어들을 보고 마스킹 된(비어 있는) 단어들을 예측하는 기법입니다. 전반적으로 BERT는 기존 Transformer의 메인 아이디어(사실 그냥 인코더)를 응용하되, 그 외의 부분을 모두 개조/발전 시켰다고 볼 수 있습니다. BERT는 공개된 후 기존 1등이었던 ELMO 모델을 밀어버리고 거의 대부분의 NLP task의 SOTA 자리를 차지했다고 합니다. (SOTA 수치들은 대부분 BERT_large 모델로 얻은 결과라고 합니다. 그래서인지 인코더를 무식하게 많이 쌓아올려 좋은 점수를 받은 것 뿐이라고 비판하는 리서처들도 있다고 함) Transformer XLBERT의 기초가 된 Transformer는 어텐션의 장점을 최대한 살리되 기존 RNN 방식을 완전히 탈피하고 주어진 시퀀스에 대한 동시 병렬 계산을 통해 Long-term dependency 문제를 해소시켜준 혁신적인 모델이었습니다. 하지만 이런 Transformer도 역시 한계를 가지고 있었는데, 시퀀스를 한번에 다 보기 때문에 시퀀스 길이가 고정적이어야 한다는 것. 그로 인해 긴 문장이 들어올 경우 문장을 잘라서(chunking) 판단해야 했고 고정 시퀀스 길이에 맞지 않는 나머지 부분은은 따로 학습이 되었습니다. 이렇게 잘라진 문장 세그먼트들은 서로 간의 정보 전달이 이뤄지지 않으므로, 너무 긴 문장에 대해서는 제대로 된 학습이 어렵습니다. Transformer가 어느 정도 긴 문장에 대한 long-term dependency은 해결했지만, 고정 시퀀스 길이를 넘는 문장에 대한 학습은 할 수 없다는 문제점의 솔루션으로서 제안된 것이 Transformer XL입니다. 아이디어는 바로 Transformer에 recurrence 추가하는 것입니다. 즉, 이전 스퀀스의 결과들을 caching하고, 그들을 현재 시퀀스의 Key와 Vector값에 넣자는 것. 여기서 사용되는 것이 relative positional encoding 이며 관련 논문이 3~4편 정도 있다고 합니다. XLNetTransformer와 BERT가 NLP계의 Game Changer로 불리며 온갖 SOTA들을 휩쓸고 다닌지 1년도 채 안되었는데, 약 두달 전 XLNet 이라는 이름의 논문이 게재되면서 다시 한번 SOTA 자리가 갈아치워지고 있다고 합니다. XLNet은 기존 NLP 모델들의 unsupervised representation learning 기법들(Autoregressive 이하 AR, Auto Encoding 이하 AE) 등을 대체할 새로운 기법으로 Permutation Language Modeling Obejective를 제안하였는데, 이를 통해 AR과 AE의 장점만을 살리고 단점은 극복할 수 있다고 설명합니다. 조금 더 설명하자면, 시퀀스 순서의 모든 permutation을 고려하고 모든 permuted factorization order들에서 모델 파라미터를 공유함으로써, BERT의 한계점이었던 독립성 가정과 pretrain-finetune 불일치 문제는 피하면서 bi-directional context는 유지할 수 있습니다. 동시에 이전 Transformer-XL에서 제시된 recurrence 추가와 relative positional encoding을 활용하여 괄목할만한 성과를 냈습니다. BERT -&gt; LM 조경현님 논문 NLP task solution들의 전체적 흐름을 정리하고 동시에 최근 활발히 연구되고 있는 주제들에 대해서 짧게 리뷰한 세션이었습니다. 특히 BERT와 XLNet에 대해 전체적인 개요를 짚어주시고 향후 NLP 연구 방향에 대한 개인적 의견도 들을 수 있어 좋았습니다." }, { "title": "순차적 데이터를 다루는 딥러닝 모델들", "url": "/blog/%EC%88%9C%EC%B0%A8%EC%A0%81-%EC%A0%95%EB%B3%B4%EB%A5%BC-%EB%8B%A4%EB%A3%A8%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EB%93%A4/", "categories": "reviews", "tags": "AI, sequential, models", "date": "2019-07-30 00:00:00 +0000", "snippet": "Recurrent Neural Networks, 줄여서 RNN의 대표 모델들 공부.IntroductionRNN은 기본적으로 순차적 정보를 가지는 sequential data(ex. time-series data) 를 효과적으로 학습하기 위해 고안된 모델이다. 데이터의 종류를 구분하는 방법은 여러가지가 있지만, 아주 간단하게는 many OR one 으로...", "content": "Recurrent Neural Networks, 줄여서 RNN의 대표 모델들 공부.IntroductionRNN은 기본적으로 순차적 정보를 가지는 sequential data(ex. time-series data) 를 효과적으로 학습하기 위해 고안된 모델이다. 데이터의 종류를 구분하는 방법은 여러가지가 있지만, 아주 간단하게는 many OR one 으로 구분할 수 있다.*[Figure 0] Sequential data and RNN위와 같이 RNN이 풀 수 있는 문제는 다양하다Short HistoryRNN 아이디어 자체는 1980년대 등장하여 근 30년간 조용히 연구되던 분야다. 기본 RNN을 계승하여 attention 기법, LSTM, GRU 등의 모델들이 차례로 등장하였고, 이 모델들의 성과는 그 당시 꽤 만족스러웠기에 Sequential data 하면 RNN을 통상적으로 사용해왔다. 특히 자연어 및 비디오/음성 데이터와 같이 순차적 성질을 가진 데이터의 경우 RNN 계열 모델을 사용하는 것이 당연했다. Then in the following years (2015–16) came ResNet and Attention. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by averagingnetworks influenced by a context vector. Reference - ‘The fall of RNN and LSTM’, 2018, Eugenio Culurciello2017년 12월에 ‘Attention is all you need’ 라는 제목의 논문이 게재되었다. 논문은 기존 RNN 기반 알고리즘을 완전히 탈피하고, sequential data를 처리할 수 있는 새로운 모델 - Transformer를 제안했다. 물론 그 이전에도 attention 기법을 기존 모델에 적용하려는 시도 및 성과는 분명 있었으나, RNN의 틀을 벗어나지는 못했었다. 논문은 그 상식을 깨고, 제목처럼 attention’만’ 써도 순차 정보를 처리하기에 충분하며 심지어 더 좋기까지 하다는 내용을 발표한 것이다. 뿐만 아니라 이들이 소개한 Transformer는 기존 SOTA 모델들을 월등한 차이로 이기며 많은 NLP task에서 좋은 점수를 냈다. 이 논문을 기점으로 NLP 분야의 연구 방향이 확 바뀌었다고 볼 수 있을 것 같은데, 이 Transformer 공부를 위해 기존 RNN 기법부터 차근차근 정리 해보려 한다.1. RNNRNN은 n번째에 대한 output #n 을 생성하기 위해, 그에 상응하는 input #n 과 추가적으로 n-1번째 hidden state #n-1 를 전달하여 문장의 순차적 특성을 활용 및 보존한다.[Figure 1.1] RNN basic architecture위는 RNN의 기본 구조를 잘 표현한 그림. 동작 방식은 대략 Input data의 순서대로,1. input x_n을 n번째 cell에 입력2. input x_n &amp; hidden state_n-1 을 cell 가중치에 곱하여 예측 값인 output h_n 을 출력3. hidden state_n 을 다음 n+1번째 cell에 전달4. input x_n+1을 n+1번째 cell에 입력5. ...반복... 입력 시퀀스가 끝날 때까지보다시피 순차적인 input-output 방식에 hidden state라는 벡터가 추가되어 동작하는데, 이 hidden state 는 순차적으로 Input -&gt; Output 계산을 해나갈 때 차례차례 전달되어 다음 연산시 함께 계산된다. n번째 input에게 이전 input(0부터 n-1번째)들의 정보를 전달하기 위함이다. Forward*[Figure 1.2] RNN Forwarding ExampleForwarding 하는 과정을 조금 더 자세히 보자. 위 그림은 ‘hell’ 이라는 순차문자열을 input으로 하였을 때, 다음에 나올 문자를 ‘o’ 라고 예측하는 예시이다. 첫번째 순서인 ‘h’ 를 첫번째 cell에 푸쉬하고 hidden layer의 연산을 통해 다음에 나올만한 문자를 확률 값으로 나타낸다. 보면 마지막 행인 ‘o’ 4.1로 가장 확률이 높지만 사실 답은 2번째 초록 글씨로 볼드처리된 ‘e’인 것을 알 수 있다. 그러니 초록글씨 행은 강화되고 빨간 글씨 행들은 약화되도록 학습시킬 것이다. 두번째 문자인 ‘e’도 같은 방식으로 연산된 후 ‘o’를 1.2의 확률로 가장 높게 예측했다. 틀렸다. 이번에도 첫번째 했던 것과 똑같은 방식으로 학습 시킬 것이다. 하지만 이번부터는 hidden layer가 동작하고 있음을 주목해야 한다. 이번에 예측된 ‘o’는 앞서 계산하였던 ‘h’에 대한 정보를 전달 받아 input 인 ‘e’ 와 함께 연산된 결과이다. 다음 cell은 다음 input인 ‘l’ 과, 전달 받은 과거의 정보(h, e)가 담긴 hidden state를 함께 연산하여 다음 문자를 예측한다. 세번째는 잘 맞췄다. 이제 마지막, 실제 정답인 ‘o’를 예측해야 하는 마지막 cell 에게 각각 상응하는 가중치와 연산된 hidden state, input을 입력 받아 다음에 나올 문자에 대한 probability를 계산한다. (+ 위 예시는 teacher forcing을 적용한 예시. 모델이 틀린 답을 냈지만 다음 cell의 input에 target char를 넣어주고 있음.) 보통, 실제 input인 ‘hell’ 에 대해 진행한 세 번의 연산은 encoder라 하고, 마지막 실제 알고자 하는 결과에 대한 연산을 decoder라 한다. 이 예제는 many to one이기에 매우 간단해 보이는데, 만약 many to many 모델이면 실제 인풋이 끝나는 지점에서 hidden state에 가중치 W_hh를 곱해준다. 또한 decoder에서 input 대신 바로 이전 cell의 output을 input으로 사용한다. *[Figure 1.3] RNN many to many architecture 2. Attention Mechanism위와 같은 RNN 구조의 가장 큰 한계점은 바로 Long-Term dependency 문제다. Long-term dependency는 문장이 길어질수록 비교적 초반부에 입력받은 input에 대한 정보를 손실하게 되는 현상을 말하는데, RNN에서 이 문제를 해결하고자 고안된 메커니즘 중 하나로 attention이 있다.*[Figure 2.1] RNN with attention 베이직한 RNN 모델은 Encoder가 먼저 순차적으로 문장을 읽어가며 하나의 hidden state 값을 전달해 나갔으나, 문장이 길어지면 비교전 초반부의 내용이 희미해지는 문제가 있다. -&gt; 그렇다면, hidden state를 여러개 만들자!베이직한 RNN 모델에서는 hidden state가 모든 input data를 거친 후 디코더의 시작 cell에 전달되기 때문에 input data의 앞 내용을 많이 잊어버린 상태일 수 밖에 없었다. 그렇기에 각 input에 대한 hidden state를 따로 저장해놓고 그 순서에 맞는 decoder cell에 전달해주자는 아이디어가 나온 것 같다. 이 방법으로 성능이 개선 되었다고는 하나, 개인적으로는 이게 attention이라는 단어와는 잘 안 맞는다는 생각이 들기도 한다. attention이라면 뭔가 주목을 해야 하는데, 이건 주목이라기 보단 그냥 초반부의 정보를 저장해놓고 활용하는 트릭인 것 같기도 하고…(내가 잘 이해한 게 맞나?)3. LSTM(Long Short-term Memory)Long-term dependency를 극복하고자 한 또다른 시도는, hidden state 계산을 업그레이드 시킨 LSTM 이다. LSTM은 기존 RNN 구조를 기반으로 하되, 이제 hidden state는 무작정 합성곱만 하는 것이 아니라, 기억해야 할 정보와 잊어버려도 될 정보들을 따로 계산하여 최대한 input 데이터의 핵심만 남기려 한다.*[Figure 3.1 ~ 3.5] LSTM Architecture전체 흐름은 기본 RNN과 동일하나, Cell state update는 크게 세가지 스텝으로 진행된다. Forward (1) Forget gate 먼저 현재 Cell-state 값을 업데이트 하기 위해 forget gate와 input gate를 각각 따로 거친다. forget gate의 수식은 위와 같다. 이전 Cell의 아웃풋(h_t-1)과 현재 인풋(x_t)를 forget layer의 Weight 및 bias 를 거쳐 시그모이드에 의해 0과 1 사이의 확률값으로 표현된다. 이렇게 표현된 값은 이전 아웃풋과 현재 인풋을 보고, 이전의 정보를 얼만큼 잊을 것인가? 를 정의한다고 볼 수 있다. 따라서 이 값은 이전 Cell state(C_t-1)과 연산한다. (2) Cell-state 업데이트 이제 현재 Cell-state를 실질적으로 업데이트 한다. 먼저 이전 아웃풋과 현재 인풋의 가중치 연산에 따라 얼마나 정보를 업데이트 할 것인지 확률값으로 나타내고(i_t), 또 다른 가중치 연산을 tanh 하여 -1 ~ 1 의 값으로 표현한 현재 Cell state의 추정치(C_t)를 구한다. 수식이 이해하기 쉽다. 마지막으로 앞서 계산한 f_t 와 이전 Cell state 값을 연산하여 이전 정보를 어느 정도 담고, 현재 추정되는 Cell state의 값을 i_t를 통해 어느 정도 담아 현재 Cell-state값을 완성한다. (3) Output 도출 현재의 Cell state 를 구했다면 이제 아웃풋 값을 계산할 수 있다. 전과 비슷한 과정으로 아웃풋이 계산된다. (그림의 x 부호는 Hadamard product 연산) matrix 연산부까지 구현한 코드 https://gist.github.com/karpathy/d4dee566867f8291f086 4. GRU(Gated Recurrent Unit)LSTM의 발전된 형태로, 성능은 유지하면서 계산량을 확 줄인 셀 구조이다. 기존 forget, input, output gate를 update gate 와 reset gate로 변형하였다. 불필요한 계산을 줄여 보다 컴팩트해진 LSTM 모델이다.*[Figure 4.1] GRU architectureLast Update: 2019/07/30Referenceshttp://colah.github.io/posts/2015-08-Understanding-LSTMs/" } ]
